{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from tqdm import trange;\n",
    "import pandas as pd;\n",
    "from matplotlib import pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy__version: 1.26.4\n",
      "Pandas__version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "print( \"NumPy__version:\", np.__version__ );\n",
    "print( \"Pandas__version:\", pd.__version__ );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 1.0 ], [ 1.0 ], [ 0.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "def load_data( name ):\n",
    "    if name == \"xor\":\n",
    "        return xor_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print( type( load_data( \"xor\" )[0][0] ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss( object ):\n",
    "    def __init__( self, name : str ):\n",
    "        self.name = name;\n",
    "\n",
    "    def __call__( self, y_pred, y_true, deriv=False ):\n",
    "        if self.name == \"mse\":\n",
    "            if deriv == False:\n",
    "                return np.mean( ( y_pred - y_true ) ** 2 );\n",
    "            else:\n",
    "                return 2 * ( y_pred - y_true ) / y_true.size;\n",
    "    \n",
    "        elif self.name == \"cross_entropy\" and deriv==False:\n",
    "            return -np.mean( y_true * np.log( y_pred ) + ( 1 - y_true ) * np.log( 1 - y_pred ) );\n",
    "\n",
    "        elif self.name == \"cross_entropy\" and deriv==True:\n",
    "            # Clip predictions to avoid division by zero\n",
    "            y_pred = np.clip( y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "            # Compute the derivative of the binary cross-entropy loss\n",
    "            derivative = (y_pred - y_true) / (y_pred * (1 - y_pred));\n",
    "            return derivative\n",
    "        \n",
    "        else:\n",
    "            raise ValueError( \"Invalid loss function\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot( Y ):\n",
    "    \"\"\"\n",
    "    return an 0 vector with 1 only in the position correspondind to the value in Y\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros( ( Y.max()+1, Y.size ) ) ;\n",
    "    one_hot_Y[Y,np.arange( Y.size )] = 1; \n",
    "    return one_hot_Y;\n",
    "\n",
    "class BackProp( object ):\n",
    "    def __init__( self ) -> None:\n",
    "        pass;\n",
    "\n",
    "    def __call__( self, data : np.ndarray , labels : np.ndarray , size : int, logits : dict , trainable_variables : dict, transfer_function : callable ) -> dict:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # compute gradients\n",
    "        grads = {};\n",
    "    \n",
    "        loss_value = np.dot( 2 , logits['output'] - labels );\n",
    "        dw2 = np.dot( 1/size, loss_value.dot( logits['hidden'].T ) );\n",
    "        db2 = 1/size * np.sum( loss_value, 1 );\n",
    "\n",
    "        dz1 = trainable_variables['W_hidden_output_layer'].T.dot( loss_value ) * transfer_function( logits['W_input_hidden_layer'], deriv=True );\n",
    "\n",
    "        dw1 = 1/size * dz1.dot( data.T );\n",
    "        db1 = 1/size * np.sum( dz1, 1 );\n",
    "\n",
    "        grads['W_hidden_output_layer'] = dw2;\n",
    "        grads['output'] = db2;\n",
    "        grads['W_input_hidden_layer'] = dw1;\n",
    "        grads['hidden'] = db1;\n",
    "\n",
    "        error = np.sum( loss_value ** 2 ) / size;\n",
    "\n",
    "        return grads, error;\n",
    "\n",
    "class GradientDescent(object):\n",
    "    \"\"\"\n",
    "    Gradient Descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters: dict):\n",
    "        self.parameters = parameters;\n",
    "        self.name = \"Gradient Descent\";\n",
    "    \n",
    "    def minimize( self, trainable_variables, grads ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for key in trainable_variables.keys():\n",
    "            if key[0] != 'W':\n",
    "                trainable_variables[key] -= self.parameters['lr'] * np.reshape( grads[key], ( self.parameters['hidden_units'],1) );\n",
    "            else:\n",
    "                x = self.parameters['lr'] * grads[key];\n",
    "                trainable_variables[key] -= x.T;\n",
    "\n",
    "        return trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer( object ):\n",
    "    \"\"\"\n",
    "    Base class for the Layer class.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name : str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Base Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.params : dict\n",
    "            self.node_no : int\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams;\n",
    "        self.name = name;\n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to connect the layers.\n",
    "\n",
    "        args:\n",
    "            other : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return NeuralNetwork( self, other );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Input layer for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Input Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.bias : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( InputLayer, self ).__init__( hyperparams, name );\n",
    "        self.node_no = hyperparams['input_units'];\n",
    "    \n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Mirror the inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs;\n",
    "        self.activated = inputs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Layer class for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str, transfer: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Layer class.\n",
    "\n",
    "        args:\n",
    "            name : str\n",
    "            params : dict\n",
    "            node_no : int\n",
    "            transfer : str\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.name : str\n",
    "            self.transfer : str\n",
    "            self.inputs : object\n",
    "            self.outputs : object\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( Layer, self ).__init__( hyperparams, name );\n",
    "        self.transfer_name = transfer;\n",
    "\n",
    "        self.inputs = None;\n",
    "        self.outputs = None;\n",
    "    \n",
    "        if \"hidden\" in name:\n",
    "            self.node_no = hyperparams['hidden_units'];\n",
    "        else:\n",
    "            self.node_no = hyperparams['output_units'];\n",
    "        \n",
    "        #create self.bias to be an array of size node_no with random values from a normal distribution between -1 and 1\n",
    "        #self.bias = np.random.rand( hyperparams['hidden_units'], hyperparams['examples'] ) - 0.5;\n",
    "        fan_in = hyperparams['hidden_units'] * hyperparams['examples'];\n",
    "        fan_out = hyperparams['hidden_units'] * hyperparams['examples'];\n",
    "        limit = np.sqrt( 6 / ( fan_in + fan_out ) );\n",
    "        self.bias = np.random.uniform( -limit, limit, ( hyperparams['hidden_units'], hyperparams['examples'] ) );\n",
    "        self.bias = self.bias.astype('float32');\n",
    "\n",
    "    def transfer_fx( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Transfer function for the layer.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.transfer_name == \"relu\" and deriv == True:\n",
    "            if deriv == True:\n",
    "                return inputs > 0;\n",
    "            else:\n",
    "                return np.maximum( 0,inputs );\n",
    "    \n",
    "        if self.transfer_name == 'softmax' and deriv == False:\n",
    "            exp = np.exp( inputs - np.max( inputs ) );\n",
    "            return exp / exp.sum( axis=0 );\n",
    "            \n",
    "        if self.transfer_name == \"sigmoid\":\n",
    "\n",
    "            if np.exp( inputs ).any() == np.inf:\n",
    "                sys.exit();\n",
    "            f_x = 1 / ( 1 + np.exp( -inputs ) );\n",
    "            if deriv == True:\n",
    "                return f_x * ( 1 - f_x );\n",
    "            else:\n",
    "                return f_x;\n",
    "\n",
    "    def output( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Calculate the output with the activation function and inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            self.outputs : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs + self.bias;\n",
    "        if deriv == True:\n",
    "            self.activated = self.transfer_fx( self.bias_add, deriv=True );\n",
    "        else:\n",
    "            self.activated = self.transfer_fx( self.bias_add );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Weight layer for the perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, src : Layer, dest : Layer ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the weight layer.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            src : Layer\n",
    "            dest : Layer\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.src : Layer\n",
    "            self.dest : Layer\n",
    "            self.input_size : tuple\n",
    "            self.output_size : tuple\n",
    "            self.name : str\n",
    "            self.weights : array\n",
    "\n",
    "        raises: \n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src = src;\n",
    "        self.dest = dest;\n",
    "    \n",
    "        #self.weights = np.random.rand( self.src.node_no, self.dest.bias.shape[0] ) - 0.5;\n",
    "        fan_in = self.src.node_no * self.dest.bias.shape[0];\n",
    "        fan_out = self.src.node_no + self.dest.bias.shape[0];\n",
    "        \n",
    "        limit = np.sqrt( 6 / ( fan_in + fan_out ) );\n",
    "        self.weights = np.random.uniform( -limit, limit, size=( self.src.node_no, self.dest.bias.shape[0] ) );\n",
    "        self.weights = self.weights.astype('float32');\n",
    "\n",
    "        self.name    = \"W_%s_%s_layer\" % ( self.src.name, self.dest.name );\n",
    "\n",
    "        self.src.outputs = self;\n",
    "        self.dest.inputs = self;\n",
    "\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Matrix multiplication between the inputs and the weights.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src.output( inputs );\n",
    "        return self.src.bias_add, self.src.activated, self.weights.T.dot( self.src.activated );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork( object ):\n",
    "    \"\"\"\n",
    "    This class respresents a Neural Networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, layer0, layer1 ):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network class.\n",
    "        Creates a network with an input layer, layer0 and an output layer, layer1.\n",
    "\n",
    "        args:\n",
    "            layer0 : object\n",
    "            layer1 : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.hyperparams          : ( dict ) hyperparameters for the network.\n",
    "            self.layers               : ( list ) list of layers in the network.\n",
    "            self.layer_name           : ( list ) list of names of the layers in the network.\n",
    "            self.input_layer          : ( object ) input layer of the network.\n",
    "            self.output_layer         : ( object ) output layer of the network.\n",
    "            self.weights              : ( list ) list of weights in the network.\n",
    "            self.weight_names         : ( list ) list of names of the weights in the network.\n",
    "            self.loss_fn              : ( object ) loss function for the network.\n",
    "            self.optimizer            : ( object ) optimizer for the network.\n",
    "            self.learnable_parameters : ( dict ) dictionary of learnable parameters in the network.\n",
    "\n",
    "        raises:\n",
    "            None \n",
    "        \"\"\"\n",
    "        # hyperparameters dictionary\n",
    "        self.hyperparams = layer0.hyperparams;\n",
    "\n",
    "        # layers\n",
    "        self.layers = [ layer0, layer1 ];\n",
    "        self.layer_name = [ layer0.name, layer1.name ];\n",
    "        self.input_layer = layer0;\n",
    "        self.output_layer = self.layers[-1];\n",
    "\n",
    "        # weights\n",
    "        self.weights = [ WeightLayer( layer0, layer1 ) ];\n",
    "        self.weight_names = [ self.weights[0].name ];\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fn = None;\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = None;\n",
    "\n",
    "        # initialize and populate learnable parameters used for learning\n",
    "        self.trainable_parameters = {};\n",
    "        for weight in self.weights:\n",
    "            self.trainable_parameters[weight.name] = weight.weights;\n",
    "        for layer in self.layers[1:]:\n",
    "            self.trainable_parameters[layer.name] = layer.bias;\n",
    "\n",
    "    \n",
    "    def summary( self ):\n",
    "        \"\"\"\n",
    "        Print the summary of the network.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print( \"-------\" );\n",
    "        print( \"| Summary |\" );\n",
    "        print( \"-------\" );\n",
    "        print( f\"Input Layer: { self.input_layer.node_no }\" );\n",
    "        print( f\"Hidden Layer: { self.layers[1].node_no }\" );\n",
    "        print( f\"Output Layer: { self.output_layer.node_no }\" );\n",
    "    \n",
    "        print( \"-------\" );\n",
    "        print( \"| Weights |\" );\n",
    "        print( \"-------\" );\n",
    "        for weight_obj in self.weights:\n",
    "            print( f\"{weight_obj.name}: \\n {weight_obj.weights} , {weight_obj.weights.shape}\" );\n",
    "        \n",
    "        print( \"-------\" );\n",
    "        print( \"| Bias |\" );\n",
    "        print( \"------\" );\n",
    "        for layer_obj in self.layers:\n",
    "            if hasattr( layer_obj, 'bias' ):\n",
    "                print( f\"{layer_obj.name}: \\n {layer_obj.bias}, {layer_obj.bias.shape}\" );\n",
    "            else:\n",
    "                print( f\"{layer_obj.name}: \" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        print( \"| Hyperparameters |\" );\n",
    "        print( \"---------------\" );\n",
    "        print( f\"Epochs: {self.hyperparams['epochs']}\" );\n",
    "        print( f\"Learning Rate: {self.hyperparams['lr']}\" );\n",
    "        print( f\"Minibatch Size: {self.hyperparams['minibatch_size']}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( f\"| Loss Function | : {self.loss_fn.name}\" );\n",
    "        print( \"---------------\" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        #print( f\"| Optimizer | : {self.optimizer.name}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( \"| Trainable Parameters | \")\n",
    "        print( \"---------------\" );\n",
    "        print( self.trainable_parameters );\n",
    "    \n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to add a layer to the network\n",
    "        \"\"\"\n",
    "        if isinstance( other, Layer ) or isinstance( other, InputLayer ):\n",
    "            # add weights between the layers, append the weight name to the list\n",
    "            self.weights.append( WeightLayer( self.layers[-1], other ) );\n",
    "            self.weight_names.append( self.weights[-1].name );\n",
    "            \n",
    "            # add the layer to the network, set the output layer to the last layer, and append the layer name to the list\n",
    "            self.layers.append( other );\n",
    "            self.output_layer = self.layers[-1];\n",
    "            self.layer_name.append( other.name );\n",
    "\n",
    "            # add the weights and biases to the trainable parameters\n",
    "            self.trainable_parameters[self.weights[-1].name] = self.weights[-1].weights;\n",
    "            self.trainable_parameters[other.name] = other.bias;\n",
    "\n",
    "            return self;\n",
    "    \n",
    "        if isinstance( other, Loss ):\n",
    "            self.loss_fn = other;\n",
    "            return self;\n",
    "\n",
    "        if isinstance( other, GradientDescent ):\n",
    "            self.optimizer = other;\n",
    "            return self;\n",
    "\n",
    "        else:\n",
    "            print( type( other ) );\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Calculate the output of the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.output_layer.output( inputs );\n",
    "\n",
    "    def forward( self, inputs ):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        fwd_output = {};\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            src_bias_add, src_activated, inputs = self.weights[i].output( inputs );\n",
    "            \n",
    "            fwd_output[self.weights[i].name] = inputs;\n",
    "            if self.layers[i].name == 'input':\n",
    "                continue;\n",
    "            fwd_output[self.layers[i].name] = src_activated;\n",
    "            fwd_output[self.layers[i].name + \"_bias_add\"] = src_bias_add;\n",
    "        \n",
    "        self.output( inputs );\n",
    "        fwd_output['output'] = self.output_layer.activated;\n",
    "        fwd_output['output_bias_add'] = self.output_layer.bias_add;\n",
    "        \n",
    "        return fwd_output;\n",
    "\n",
    "    def predict( self, X ):\n",
    "        \"\"\"\n",
    "        Predict the output of the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logits = self.forward( X );\n",
    "        return np.argmax( logits['output'], 0 );\n",
    "\n",
    "    def get_accuracy( self, predictions, Y ):\n",
    "        return np.sum( predictions == Y )/Y.size\n",
    "\n",
    "    def train( self, X, y ):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "            y : array\n",
    "            epochs : int\n",
    "            lr : float\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.losses_pd = pd.DataFrame( columns = ['fwd_losses'], index=[0] );\n",
    "        self.weights_pd = pd.DataFrame();\n",
    "        self.bias_pd = pd.DataFrame();\n",
    "        self.weight_layer_output_pd = pd.DataFrame();\n",
    "        self.activated_pd  = pd.DataFrame();\n",
    "        self.grad_pd = pd.DataFrame();\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            weights = self.weights[i];\n",
    "            for l in range ( weights.weights.shape[0] ):\n",
    "                for j in range( weights.weights.shape[1] ):\n",
    "                    self.weights_pd.loc[ 0 ,  weights.name + f\"{l}_{j}\" ] = weights.weights[l][j];\n",
    "        \n",
    "        for i in range( len( self.layers ) ):\n",
    "            layer = self.layers[i];\n",
    "            if hasattr( layer, 'bias' ):\n",
    "                for j in range( layer.bias.shape[0] ):\n",
    "                    self.bias_pd.loc[ 0, layer.name + f\"{j}\" ] = layer.bias[j];\n",
    "        \n",
    "        \n",
    "        #print( self.weights_pd )\n",
    "        #print( self.bias_pd )\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in trange( self.hyperparams['epochs'] ):\n",
    "  \n",
    "            # forward pass\n",
    "            #z1, a1, z2, a2 = self.forward( X );\n",
    "            fwd_outputs = self.forward( X );\n",
    "\n",
    "            #for item in fwd_outputs:\n",
    "                #print( item, fwd_outputs[item].shape );\n",
    "\n",
    "            # logging\n",
    "            #for name in self.weight_names:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                        #self.weight_layer_output_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "                \n",
    "            #for name in self.layer_name[1:]:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                       # self.activated_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "        \n",
    "\n",
    "            # backward pass\n",
    "            #one_hot_labels = one_hot( y )\n",
    "            size = y.size;\n",
    "            \n",
    "            grads, error = BackProp()( X, y, size, fwd_outputs, self.trainable_parameters, self.layers[1].transfer_fx );\n",
    "            \n",
    "            # update the weights\n",
    "            self.trainable_parameters = self.optimizer.minimize( self.trainable_parameters, grads );\n",
    "\n",
    "            # update the weights and biases\n",
    "            for weight in self.weights:\n",
    "                weight.weights = self.trainable_parameters[weight.name];\n",
    "\n",
    "            for layer in self.layers[1:]:\n",
    "                layer.bias = self.trainable_parameters[layer.name];\n",
    "\n",
    "        print( f\"Epoch: {epoch+1} / {self.hyperparams['epochs']}, Error: {error}\" );\n",
    "        prediction = self.predict( X );\n",
    "        print( f\"Accuracy: {self.get_accuracy( prediction, y )}\" );\n",
    "\n",
    "            #if ( epoch+1 ) % int( self.hyperparams['epochs']/10 ) == 0:\n",
    "                #print(f\"Iteration: {epoch+1} / {self.hyperparams['epochs']}\")\n",
    "                #prediction = self.predict( X );\n",
    "                #print(f'{self.get_accuracy(prediction, y):.3%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = load_data( \"xor\" );\n",
    "x_train, y_train = training[0].T, training[1].T;\n",
    "x_test, y_test = testing[0].T, testing[1].T;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    #seed\n",
    "    \"seed\" : 42,\n",
    "\n",
    "    \"examples\" : x_train.shape[1], \n",
    "    \n",
    "    # model hyperparameters\n",
    "    \"input_units\" : x_train.shape[0],\n",
    "    \"hidden_units\" : 2,\n",
    "    \"output_units\" : 1,\n",
    "\n",
    "    # optimizer hyperparameters\n",
    "    \"lr\" : 0.55,\n",
    "\n",
    "    # training hyperparameters\n",
    "    \"epochs\" : 1,\n",
    "    \"minibatch_size\" : 4\n",
    "}\n",
    "#np.random.seed(params['seed']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = InputLayer( params, \"input\" );\n",
    "hidden_layer = Layer( params, \"hidden\", \"sigmoid\" );\n",
    "output_layer = Layer( params, \"output\", \"sigmoid\" );\n",
    "\n",
    "loss = Loss( \"mse\" );\n",
    "optimizer = GradientDescent( params );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = input_layer >> hidden_layer >> output_layer >> loss >> optimizer;\n",
    "#nn.summary();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1289.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 1, Error: 2.2212015722528387\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nn.train( x_train, y_train );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print( nn.predict( x_test ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
