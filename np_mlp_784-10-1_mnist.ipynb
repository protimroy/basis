{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from tqdm import trange;\n",
    "import pandas as pd;\n",
    "from keras.datasets import mnist;\n",
    "import sys;\n",
    "from matplotlib import pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy__version: 1.26.4\n",
      "Pandas__version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "print( \"NumPy__version:\", np.__version__ );\n",
    "print( \"Pandas__version:\", pd.__version__ );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss( object ):\n",
    "    def __init__( self, name : str ):\n",
    "        self.name = name;\n",
    "\n",
    "    def __call__( self, y_pred, y_true, deriv=False ):\n",
    "        if self.name == \"mse\" and deriv==False:\n",
    "            return np.mean( ( y_pred - y_true ) ** 2 );\n",
    "    \n",
    "        elif self.name == \"cross_entropy\" and deriv==False:\n",
    "            return -np.mean( y_true * np.log( y_pred ) + ( 1 - y_true ) * np.log( 1 - y_pred ) );\n",
    "\n",
    "        elif self.name == \"cross_entropy\" and deriv==True:\n",
    "            # Clip predictions to avoid division by zero\n",
    "            #y_pred = np.clip( y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "            # Compute the derivative of the binary cross-entropy loss\n",
    "            #N = y_true.shape[0]\n",
    "            \n",
    "            derivative = ( y_pred - y_true ) / ( y_pred * ( 1 - y_pred ) );\n",
    "            return derivative\n",
    "        \n",
    "        else:\n",
    "            raise ValueError( \"Invalid loss function\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot( Y ):\n",
    "    \"\"\"\n",
    "    return an 0 vector with 1 only in the position correspondind to the value in Y\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros( ( Y.max()+1, Y.size ) ) ;\n",
    "    one_hot_Y[Y,np.arange( Y.size )] = 1; \n",
    "    return one_hot_Y;\n",
    "\n",
    "class BackProp( object ):\n",
    "    def __init__( self ) -> None:\n",
    "        pass;\n",
    "\n",
    "    def __call__( self, data : np.ndarray , labels : np.ndarray , size : int, logits : dict , trainable_variables : dict, transfer_function : callable ) -> dict:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # compute gradients\n",
    "        grads = {};\n",
    "    \n",
    "        loss_value = np.dot( 2 , logits['output'] - labels );\n",
    "        dw2 = np.dot( 1/size, loss_value.dot( logits['hidden'].T ) );\n",
    "        db2 = 1/size * np.sum( loss_value, 1 );\n",
    "\n",
    "        \n",
    "        dz1 = trainable_variables['W_hidden_output_layer'].T.dot( loss_value ) * transfer_function( logits['W_input_hidden_layer'], deriv=True );\n",
    "\n",
    "\n",
    "        dw1 = 1/size * dz1.dot( data.T );\n",
    "        db1 = 1/size * np.sum( dz1, 1 );\n",
    "\n",
    "        grads['W_hidden_output_layer'] = dw2;\n",
    "        grads['output'] = db2;\n",
    "        grads['W_input_hidden_layer'] = dw1;\n",
    "        grads['hidden'] = db1;\n",
    "\n",
    "        return grads;\n",
    "\n",
    "class GradientDescent(object):\n",
    "    \"\"\"\n",
    "    Gradient Descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters: dict):\n",
    "        self.parameters = parameters;\n",
    "        self.name = \"Gradient Descent\";\n",
    "    \n",
    "    def minimize( self, trainable_variables, grads ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for key in trainable_variables.keys():\n",
    "            if key[0] != 'W':\n",
    "                trainable_variables[key] -= self.parameters['lr'] * np.reshape( grads[key], ( self.parameters['hidden_units'],1) );\n",
    "            else:\n",
    "                x = self.parameters['lr'] * grads[key];\n",
    "                trainable_variables[key] -= x.T;\n",
    "\n",
    "        return trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer( object ):\n",
    "    \"\"\"\n",
    "    Base class for the Layer class.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name : str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Base Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.params : dict\n",
    "            self.node_no : int\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams;\n",
    "        self.name = name;\n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to connect the layers.\n",
    "\n",
    "        args:\n",
    "            other : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return NeuralNetwork( self, other );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Input layer for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Input Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.bias : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( InputLayer, self ).__init__( hyperparams, name );\n",
    "        self.node_no = hyperparams['input_units'];\n",
    "    \n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Mirror the inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs;\n",
    "        self.activated = inputs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Layer class for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str, transfer: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Layer class.\n",
    "\n",
    "        args:\n",
    "            name : str\n",
    "            params : dict\n",
    "            node_no : int\n",
    "            transfer : str\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.name : str\n",
    "            self.transfer : str\n",
    "            self.inputs : object\n",
    "            self.outputs : object\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( Layer, self ).__init__( hyperparams, name );\n",
    "        self.transfer_name = transfer;\n",
    "\n",
    "        self.inputs = None;\n",
    "        self.outputs = None;\n",
    "    \n",
    "        if \"hidden\" in name:\n",
    "            self.node_no = hyperparams['hidden_units'];\n",
    "        else:\n",
    "            self.node_no = hyperparams['output_units'];\n",
    "        \n",
    "        #create self.bias to be an array of size node_no with random values from a normal distribution between -1 and 1\n",
    "        self.bias = np.random.rand( hyperparams['hidden_units'], hyperparams['examples']) - 0.5;\n",
    "        \n",
    "\n",
    "    def transfer_fx( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Transfer function for the layer.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.transfer_name == \"relu\":\n",
    "            if deriv == True:\n",
    "                return inputs > 0;\n",
    "            else:\n",
    "                return np.maximum( 0,inputs );\n",
    "    \n",
    "        if self.transfer_name == 'softmax' and deriv == False:\n",
    "            exp = np.exp( inputs - np.max( inputs ) );\n",
    "            return exp / exp.sum( axis=0 );\n",
    "            \n",
    "        if self.transfer_name == \"sigmoid\":\n",
    "            f_x = 1 / ( 1 + np.exp( -inputs ) );\n",
    "            if deriv == True:\n",
    "                return f_x * ( 1 - f_x );\n",
    "            else:\n",
    "                return f_x;\n",
    "\n",
    "\n",
    "    def output( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Calculate the output with the activation function and inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            self.outputs : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs + self.bias;\n",
    "        if deriv == True:\n",
    "            self.activated = self.transfer_fx( self.bias_add, deriv=True );\n",
    "        else:\n",
    "            self.activated = self.transfer_fx( self.bias_add );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Weight layer for the perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, src : Layer, dest : Layer ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the weight layer.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            src : Layer\n",
    "            dest : Layer\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.src : Layer\n",
    "            self.dest : Layer\n",
    "            self.input_size : tuple\n",
    "            self.output_size : tuple\n",
    "            self.name : str\n",
    "            self.weights : array\n",
    "\n",
    "        raises: \n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src = src;\n",
    "        self.dest = dest;\n",
    "    \n",
    "        self.weights = np.random.rand(self.src.node_no, self.dest.bias.shape[0]) - 0.5;\n",
    "\n",
    "        self.name    = \"W_%s_%s_layer\" % ( self.src.name, self.dest.name );\n",
    "\n",
    "        self.src.outputs = self;\n",
    "        self.dest.inputs = self;\n",
    "\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Matrix multiplication between the inputs and the weights.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src.output( inputs );\n",
    "        return self.src.bias_add, self.src.activated, self.weights.T.dot( self.src.activated );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork( object ):\n",
    "    \"\"\"\n",
    "    This class respresents a Neural Networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, layer0, layer1 ):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network class.\n",
    "        Creates a network with an input layer, layer0 and an output layer, layer1.\n",
    "\n",
    "        args:\n",
    "            layer0 : object\n",
    "            layer1 : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.hyperparams          : ( dict ) hyperparameters for the network.\n",
    "            self.layers               : ( list ) list of layers in the network.\n",
    "            self.layer_name           : ( list ) list of names of the layers in the network.\n",
    "            self.input_layer          : ( object ) input layer of the network.\n",
    "            self.output_layer         : ( object ) output layer of the network.\n",
    "            self.weights              : ( list ) list of weights in the network.\n",
    "            self.weight_names         : ( list ) list of names of the weights in the network.\n",
    "            self.loss_fn              : ( object ) loss function for the network.\n",
    "            self.optimizer            : ( object ) optimizer for the network.\n",
    "            self.learnable_parameters : ( dict ) dictionary of learnable parameters in the network.\n",
    "\n",
    "        raises:\n",
    "            None \n",
    "        \"\"\"\n",
    "        # hyperparameters dictionary\n",
    "        self.hyperparams = layer0.hyperparams;\n",
    "\n",
    "        # layers\n",
    "        self.layers = [ layer0, layer1 ];\n",
    "        self.layer_name = [ layer0.name, layer1.name ];\n",
    "        self.input_layer = layer0;\n",
    "        self.output_layer = self.layers[-1];\n",
    "\n",
    "        # weights\n",
    "        self.weights = [ WeightLayer( layer0, layer1 ) ];\n",
    "        self.weight_names = [ self.weights[0].name ];\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fn = None;\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = None;\n",
    "\n",
    "        # initialize and populate learnable parameters used for learning\n",
    "        self.trainable_parameters = {};\n",
    "        for weight in self.weights:\n",
    "            self.trainable_parameters[weight.name] = weight.weights;\n",
    "        for layer in self.layers[1:]:\n",
    "            self.trainable_parameters[layer.name] = layer.bias;\n",
    "\n",
    "    \n",
    "    def summary( self ):\n",
    "        \"\"\"\n",
    "        Print the summary of the network.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print( \"-------\" );\n",
    "        print( \"| Summary |\" );\n",
    "        print( \"-------\" );\n",
    "        print( f\"Input Layer: { self.input_layer.node_no }\" );\n",
    "        print( f\"Hidden Layer: { self.layers[1].node_no }\" );\n",
    "        print( f\"Output Layer: { self.output_layer.node_no }\" );\n",
    "    \n",
    "        print( \"-------\" );\n",
    "        print( \"| Weights |\" );\n",
    "        print( \"-------\" );\n",
    "        for weight_obj in self.weights:\n",
    "            print( f\"{weight_obj.name}: \\n {weight_obj.weights} , {weight_obj.weights.shape}\" );\n",
    "        \n",
    "        print( \"-------\" );\n",
    "        print( \"| Bias |\" );\n",
    "        print( \"------\" );\n",
    "        for layer_obj in self.layers:\n",
    "            if hasattr( layer_obj, 'bias' ):\n",
    "                print( f\"{layer_obj.name}: \\n {layer_obj.bias}, {layer_obj.bias.shape}\" );\n",
    "            else:\n",
    "                print( f\"{layer_obj.name}: \" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        print( \"| Hyperparameters |\" );\n",
    "        print( \"---------------\" );\n",
    "        print( f\"Epochs: {self.hyperparams['epochs']}\" );\n",
    "        print( f\"Learning Rate: {self.hyperparams['lr']}\" );\n",
    "        print( f\"Minibatch Size: {self.hyperparams['minibatch_size']}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( f\"| Loss Function | : {self.loss_fn.name}\" );\n",
    "        print( \"---------------\" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        #print( f\"| Optimizer | : {self.optimizer.name}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( \"| Trainable Parameters | \")\n",
    "        print( \"---------------\" );\n",
    "        print( self.trainable_parameters );\n",
    "    \n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to add a layer to the network\n",
    "        \"\"\"\n",
    "        if isinstance( other, Layer ) or isinstance( other, InputLayer ):\n",
    "            # add weights between the layers, append the weight name to the list\n",
    "            self.weights.append( WeightLayer( self.layers[-1], other ) );\n",
    "            self.weight_names.append( self.weights[-1].name );\n",
    "            \n",
    "            # add the layer to the network, set the output layer to the last layer, and append the layer name to the list\n",
    "            self.layers.append( other );\n",
    "            self.output_layer = self.layers[-1];\n",
    "            self.layer_name.append( other.name );\n",
    "\n",
    "            # add the weights and biases to the trainable parameters\n",
    "            self.trainable_parameters[self.weights[-1].name] = self.weights[-1].weights;\n",
    "            self.trainable_parameters[other.name] = other.bias;\n",
    "\n",
    "            return self;\n",
    "    \n",
    "        if isinstance( other, Loss ):\n",
    "            self.loss_fn = other;\n",
    "            return self;\n",
    "\n",
    "        if isinstance( other, GradientDescent ):\n",
    "            self.optimizer = other;\n",
    "            return self;\n",
    "\n",
    "        else:\n",
    "            print( type( other ) );\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Calculate the output of the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.output_layer.output( inputs );\n",
    "\n",
    "    def forward( self, inputs ):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        fwd_output = {};\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            src_bias_add, src_activated, inputs = self.weights[i].output( inputs );\n",
    "            \n",
    "            fwd_output[self.weights[i].name] = inputs;\n",
    "            if self.layers[i].name == 'input':\n",
    "                continue;\n",
    "            fwd_output[self.layers[i].name] = src_activated;\n",
    "            fwd_output[self.layers[i].name + \"_bias_add\"] = src_bias_add;\n",
    "        \n",
    "        self.output( inputs );\n",
    "        fwd_output['output'] = self.output_layer.activated;\n",
    "        fwd_output['output_bias_add'] = self.output_layer.bias_add;\n",
    "        \n",
    "        return fwd_output;\n",
    "\n",
    "    def predict( self, X ):\n",
    "        \"\"\"\n",
    "        Predict the output of the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logits = self.forward( X );\n",
    "        return np.argmax( logits['output'], 0 );\n",
    "\n",
    "    def get_accuracy( self, predictions, Y ):\n",
    "        return np.sum( predictions == Y )/Y.size\n",
    "\n",
    "    def train( self, X, y ):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "            y : array\n",
    "            epochs : int\n",
    "            lr : float\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.losses_pd = pd.DataFrame( columns = ['fwd_losses'], index=[0] );\n",
    "        self.weights_pd = pd.DataFrame();\n",
    "        self.bias_pd = pd.DataFrame();\n",
    "        self.weight_layer_output_pd = pd.DataFrame();\n",
    "        self.activated_pd  = pd.DataFrame();\n",
    "        self.grad_pd = pd.DataFrame();\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            weights = self.weights[i];\n",
    "            for l in range ( weights.weights.shape[0] ):\n",
    "                for j in range( weights.weights.shape[1] ):\n",
    "                    self.weights_pd.loc[ 0 ,  weights.name + f\"{l}_{j}\" ] = weights.weights[l][j];\n",
    "        \n",
    "        for i in range( len( self.layers ) ):\n",
    "            layer = self.layers[i];\n",
    "            if hasattr( layer, 'bias' ):\n",
    "                for j in range( layer.bias.shape[0] ):\n",
    "                    self.bias_pd.loc[ 0, layer.name + f\"{j}\" ] = layer.bias[j];\n",
    "        \n",
    "        \n",
    "        #print( self.weights_pd )\n",
    "        #print( self.bias_pd )\n",
    "        \"\"\"\n",
    "        one_hot_labels = one_hot( y )\n",
    "        size = y.size;\n",
    "\n",
    "        for epoch in range( self.hyperparams['epochs'] ):\n",
    "  \n",
    "            # forward pass\n",
    "            fwd_outputs = self.forward( X );\n",
    "\n",
    "            #for item in fwd_outputs:\n",
    "                #print( item, fwd_outputs[item].shape );\n",
    "\n",
    "            # logging\n",
    "            #for name in self.weight_names:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                        #self.weight_layer_output_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "                \n",
    "            #for name in self.layer_name[1:]:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                       # self.activated_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "        \n",
    "\n",
    "            # backward pass\n",
    "            grads = BackProp()( X, one_hot_labels, size, fwd_outputs, self.trainable_parameters, self.layers[1].transfer_fx );\n",
    "            \n",
    "            # update the weights\n",
    "            self.trainable_parameters = self.optimizer.minimize( self.trainable_parameters, grads );\n",
    "\n",
    "            # update the weights and biases\n",
    "            for weight in self.weights:\n",
    "                weight.weights = self.trainable_parameters[weight.name];\n",
    "\n",
    "            for layer in self.layers[1:]:\n",
    "                layer.bias = self.trainable_parameters[layer.name];\n",
    "\n",
    "\n",
    "            if ( epoch+1 ) % int( self.hyperparams['epochs']/10 ) == 0:\n",
    "                print(f\"Iteration: {epoch+1} / {self.hyperparams['epochs']}\")\n",
    "                prediction = self.predict( X );\n",
    "                print(f'{self.get_accuracy(prediction, y):.3%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255 # TRES IMPORTANT SINON OVERFLOW SUR EXP\n",
    "WIDTH = x_train.shape[1]\n",
    "HEIGHT = x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
    "x_test = x_test.reshape(x_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\n",
    "\n",
    "print( x_train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    #seed\n",
    "    \"seed\" : 42,\n",
    "\n",
    "    \"examples\" : x_train.shape[1],\n",
    "    \n",
    "    # model hyperparameters\n",
    "    \"input_units\" : x_train.shape[0],\n",
    "    \"hidden_units\" : 10,\n",
    "    \"output_units\" : 1,\n",
    "\n",
    "    # optimizer hyperparameters\n",
    "    \"lr\" : 0.15,\n",
    "\n",
    "    # training hyperparameters\n",
    "    \"epochs\" : 350,\n",
    "    \"minibatch_size\" : 4\n",
    "}\n",
    "#np.random.seed(params['seed']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = InputLayer( params, \"input\" );\n",
    "hidden_layer = Layer( params, \"hidden\", \"sigmoid\" );\n",
    "output_layer = Layer( params, \"output\", \"softmax\" );\n",
    "\n",
    "loss = Loss( \"cross_entropy\" );\n",
    "optimizer = GradientDescent( params );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = input_layer >> output_layer >> loss >> optimizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[0;32mIn[85], line 292\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    274\u001b[0m fwd_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward( X );\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m#for item in fwd_outputs:\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m#print( item, fwd_outputs[item].shape );\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mBackProp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransfer_fx\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# update the weights\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_parameters, grads );\n",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m, in \u001b[0;36mBackProp.__call__\u001b[0;34m(self, data, labels, size, logits, trainable_variables, transfer_function)\u001b[0m\n\u001b[1;32m     17\u001b[0m grads \u001b[38;5;241m=\u001b[39m {};\n\u001b[1;32m     19\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot( \u001b[38;5;241m2\u001b[39m , logits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m labels );\n\u001b[0;32m---> 20\u001b[0m dw2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot( \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39msize, loss_value\u001b[38;5;241m.\u001b[39mdot( \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT ) );\n\u001b[1;32m     21\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39msize \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum( loss_value, \u001b[38;5;241m1\u001b[39m );\n\u001b[1;32m     24\u001b[0m dz1 \u001b[38;5;241m=\u001b[39m trainable_variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_hidden_output_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot( loss_value ) \u001b[38;5;241m*\u001b[39m transfer_function( logits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_input_hidden_layer\u001b[39m\u001b[38;5;124m'\u001b[39m], deriv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m );\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hidden'"
     ]
    }
   ],
   "source": [
    "nn.train( x_train, y_train );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [5 3 3 ... 3 3 5]\n",
      "Unique values:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Counts:  [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "vec = x_train[:, index,None]\n",
    "label = y_train[index]\n",
    "prediction = nn.predict(vec)\n",
    "\n",
    "print(\"Prediction: \", prediction)\n",
    "\n",
    "# get the unique values in prediction\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Unique values: \", unique)\n",
    "print(\"Counts: \", counts)\n",
    "\n",
    "print(\"Label: \", label)\n",
    "\n",
    "current_image = vec.reshape((WIDTH, HEIGHT)) * SCALE_FACTOR\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(current_image, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
