{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from tqdm import trange;\n",
    "import pandas as pd;\n",
    "from keras.datasets import mnist;\n",
    "import sys;\n",
    "from matplotlib import pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy__version: 1.26.4\n",
      "Pandas__version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "print( \"NumPy__version:\", np.__version__ );\n",
    "print( \"Pandas__version:\", pd.__version__ );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 1.0 ], [ 1.0 ], [ 0.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "def load_data( name ):\n",
    "    if name == \"xor\":\n",
    "        return xor_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print( type( load_data( \"xor\" )[0][0] ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss( object ):\n",
    "    def __init__( self, name : str ):\n",
    "        self.name = name;\n",
    "\n",
    "    def __call__( self, y_pred, y_true, deriv=False ):\n",
    "        if self.name == \"mse\" and deriv==False:\n",
    "            return np.mean( ( y_pred - y_true ) ** 2 );\n",
    "    \n",
    "        elif self.name == \"cross_entropy\" and deriv==False:\n",
    "            return -np.mean( y_true * np.log( y_pred ) + ( 1 - y_true ) * np.log( 1 - y_pred ) );\n",
    "\n",
    "        elif self.name == \"cross_entropy\" and deriv==True:\n",
    "            # Clip predictions to avoid division by zero\n",
    "            #y_pred = np.clip( y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "            # Compute the derivative of the binary cross-entropy loss\n",
    "            #N = y_true.shape[0]\n",
    "            \n",
    "            derivative = (y_pred - y_true) / (y_pred * (1 - y_pred));\n",
    "            return derivative\n",
    "        \n",
    "        else:\n",
    "            raise ValueError( \"Invalid loss function\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot( Y ):\n",
    "    \"\"\"\n",
    "    return an 0 vector with 1 only in the position correspondind to the value in Y\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros( ( Y.size, Y.max() + 1 ) );\n",
    "    one_hot[ np.arange( Y.size ), Y ] = 1;\n",
    "    return one_hot;\n",
    "\n",
    "class BackProp( object ):\n",
    "    def __init__( self ) -> None:\n",
    "        pass;\n",
    "\n",
    "    def __call__( self, data : np.ndarray , labels : np.ndarray , size : int, logits : dict , trainable_variables : dict, transfer_function : callable ) -> dict:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # compute gradients\n",
    "        grads = {};\n",
    "        \n",
    "        #loss_value = 2 * ( a2 - one_hot_labels );\n",
    "        #loss_value = 2 * ( logits['output'] - labels );\n",
    "        loss_value = np.dot( 2 , logits['output'] - labels );\n",
    "        \n",
    "        dw2 = np.dot( 1/size, loss_value.dot( logits['hidden'].T ) );\n",
    "        db2 = 1/size * np.sum( loss_value, 1 );\n",
    "\n",
    "        dz1 = trainable_variables['W_hidden_output_layer'].T.dot( loss_value ) * transfer_function( logits['W_input_hidden_layer'], deriv=True );\n",
    "\n",
    "        dw1 = 1/size * dz1.dot( data.T );\n",
    "        db1 = 1/size * np.sum( dz1, 1 );\n",
    "\n",
    "        grads['W_hidden_output_layer'] = dw2;\n",
    "        grads['output'] = db2;\n",
    "        grads['W_input_hidden_layer'] = dw1;\n",
    "        grads['hidden'] = db1;\n",
    "\n",
    "        return grads;\n",
    "\n",
    "class GradientDescent(object):\n",
    "    \"\"\"\n",
    "    Gradient Descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters: dict):\n",
    "        self.parameters = parameters;\n",
    "        self.name = \"Gradient Descent\";\n",
    "    \n",
    "    def minimize( self, trainable_variables, grads ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for key in trainable_variables.keys():\n",
    "            if key[0] != 'W':\n",
    "                trainable_variables[key] -= self.parameters['lr'] * np.reshape( grads[key], (10,1) );\n",
    "            else:\n",
    "                x = self.parameters['lr'] * grads[key];\n",
    "                trainable_variables[key] -= x.T;\n",
    "\n",
    "        return trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer( object ):\n",
    "    \"\"\"\n",
    "    Base class for the Layer class.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name : str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Base Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.params : dict\n",
    "            self.node_no : int\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams;\n",
    "        self.name = name;\n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to connect the layers.\n",
    "\n",
    "        args:\n",
    "            other : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return NeuralNetwork( self, other );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Input layer for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Input Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.bias : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( InputLayer, self ).__init__( hyperparams, name );\n",
    "        self.node_no = hyperparams['input_units'];\n",
    "    \n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Mirror the inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs;\n",
    "        self.activated = inputs;\n",
    "        #return self.bias_add, self.activated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Layer class for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str, transfer: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Layer class.\n",
    "\n",
    "        args:\n",
    "            name : str\n",
    "            params : dict\n",
    "            node_no : int\n",
    "            transfer : str\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.name : str\n",
    "            self.transfer : str\n",
    "            self.inputs : object\n",
    "            self.outputs : object\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( Layer, self ).__init__( hyperparams, name );\n",
    "        self.transfer_name = transfer;\n",
    "\n",
    "        self.inputs = None;\n",
    "        self.outputs = None;\n",
    "    \n",
    "        if \"hidden\" in name:\n",
    "            self.node_no = hyperparams['hidden_units'];\n",
    "        else:\n",
    "            self.node_no = hyperparams['output_units'];\n",
    "        \n",
    "        #create self.bias to be an array of size node_no with random values from a normal distribution between -1 and 1\n",
    "        self.bias = np.random.rand( hyperparams['hidden_units'], hyperparams['examples'] ) - 0.5;\n",
    "        #self.bias = np.random.randn( self.node_no, ) - 0.5\n",
    "        #self.bias = np.random.uniform( -1, 1, ( self.node_no, 1 ) );\n",
    "        \n",
    "    \n",
    "    def transfer_fx( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Transfer function for the layer.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.transfer_name == \"relu\" and deriv == True:\n",
    "            return inputs > 0;\n",
    "    \n",
    "        # implement the relu transfer function\n",
    "        if self.transfer_name == \"relu\" and deriv == False:\n",
    "            return np.maximum( 0,inputs );\n",
    "        \n",
    "        if self.transfer_name == 'softmax' and deriv == False:\n",
    "            exp = np.exp( inputs - np.max( inputs ) );\n",
    "            return exp / exp.sum( axis=0 );\n",
    "            \n",
    "        if self.transfer_name == \"sigmoid\" and deriv == True:\n",
    "            return inputs * ( 1 - inputs );\n",
    "    \n",
    "        # implement the sigmoid transfer function\n",
    "        if self.transfer_name == \"sigmoid\" and deriv == False:\n",
    "            return 1 / ( 1 + np.exp( -inputs ) );\n",
    "\n",
    "\n",
    "    def output( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Calculate the output with the activation function and inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            self.outputs : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.bias_add = inputs + self.bias;\n",
    "        if deriv == True:\n",
    "            self.activated = self.transfer_fx( self.bias_add, deriv=True );\n",
    "        else:\n",
    "            self.activated = self.transfer_fx( self.bias_add );\n",
    "        #return self.bias_add, self.activated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Weight layer for the perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, src : Layer, dest : Layer ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the weight layer.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            src : Layer\n",
    "            dest : Layer\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.src : Layer\n",
    "            self.dest : Layer\n",
    "            self.input_size : tuple\n",
    "            self.output_size : tuple\n",
    "            self.name : str\n",
    "            self.weights : array\n",
    "\n",
    "        raises: \n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src = src;\n",
    "        self.dest = dest;\n",
    "    \n",
    "        self.weights = np.random.rand( self.src.node_no, self.dest.bias.shape[0] ) - 0.5;\n",
    "        #self.weights = np.random.uniform( -1, 1, ( self.src.node_no, self.dest.bias.shape[0] ) );\n",
    "        self.name    = \"W_%s_%s_layer\" % ( self.src.name, self.dest.name );\n",
    "\n",
    "        self.src.outputs = self;\n",
    "        self.dest.inputs = self;\n",
    "\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Matrix multiplication between the inputs and the weights.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        #return self.src.output( inputs ) @ self.weights;\n",
    "        #return self.weights.dot( self.src.output( inputs ) );\n",
    "        \n",
    "        #prev_layer_bias_add, prev_layer_activated = self.src.output( inputs );\n",
    "        #return prev_layer_bias_add, prev_layer_activated, self.weights.T.dot( prev_layer_activated );\n",
    "        self.src.output( inputs );\n",
    "        return self.src.bias_add, self.src.activated, self.weights.T.dot( self.src.activated );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork( object ):\n",
    "    \"\"\"\n",
    "    This class respresents a Neural Networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, layer0, layer1 ):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network class.\n",
    "        Creates a network with an input layer, layer0 and an output layer, layer1.\n",
    "\n",
    "        args:\n",
    "            layer0 : object\n",
    "            layer1 : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.hyperparams          : ( dict ) hyperparameters for the network.\n",
    "            self.layers               : ( list ) list of layers in the network.\n",
    "            self.layer_name           : ( list ) list of names of the layers in the network.\n",
    "            self.input_layer          : ( object ) input layer of the network.\n",
    "            self.output_layer         : ( object ) output layer of the network.\n",
    "            self.weights              : ( list ) list of weights in the network.\n",
    "            self.weight_names         : ( list ) list of names of the weights in the network.\n",
    "            self.loss_fn              : ( object ) loss function for the network.\n",
    "            self.optimizer            : ( object ) optimizer for the network.\n",
    "            self.learnable_parameters : ( dict ) dictionary of learnable parameters in the network.\n",
    "\n",
    "        raises:\n",
    "            None \n",
    "        \"\"\"\n",
    "        # hyperparameters dictionary\n",
    "        self.hyperparams = layer0.hyperparams;\n",
    "\n",
    "        # layers\n",
    "        self.layers = [ layer0, layer1 ];\n",
    "        self.layer_name = [ layer0.name, layer1.name ];\n",
    "        self.input_layer = layer0;\n",
    "        self.output_layer = self.layers[-1];\n",
    "\n",
    "        # weights\n",
    "        self.weights = [ WeightLayer( layer0, layer1 ) ];\n",
    "        self.weight_names = [ self.weights[0].name ];\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fn = None;\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = None;\n",
    "\n",
    "        # initialize and populate learnable parameters used for learning\n",
    "        self.trainable_parameters = {};\n",
    "        for weight in self.weights:\n",
    "            self.trainable_parameters[weight.name] = weight.weights;\n",
    "        for layer in self.layers[1:]:\n",
    "            self.trainable_parameters[layer.name] = layer.bias;\n",
    "\n",
    "    \n",
    "    def summary( self ):\n",
    "        \"\"\"\n",
    "        Print the summary of the network.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print( \"-------\" );\n",
    "        print( \"| Summary |\" );\n",
    "        print( \"-------\" );\n",
    "        print( f\"Input Layer: { self.input_layer.node_no }\" );\n",
    "        print( f\"Hidden Layer: { self.layers[1].node_no }\" );\n",
    "        print( f\"Output Layer: { self.output_layer.node_no }\" );\n",
    "    \n",
    "        print( \"-------\" );\n",
    "        print( \"| Weights |\" );\n",
    "        print( \"-------\" );\n",
    "        for weight_obj in self.weights:\n",
    "            print( f\"{weight_obj.name}: \\n {weight_obj.weights} , {weight_obj.weights.shape}\" );\n",
    "        \n",
    "        print( \"-------\" );\n",
    "        print( \"| Bias |\" );\n",
    "        print( \"------\" );\n",
    "        for layer_obj in self.layers:\n",
    "            if hasattr( layer_obj, 'bias' ):\n",
    "                print( f\"{layer_obj.name}: \\n {layer_obj.bias}, {layer_obj.bias.shape}\" );\n",
    "            else:\n",
    "                print( f\"{layer_obj.name}: \" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        print( \"| Hyperparameters |\" );\n",
    "        print( \"---------------\" );\n",
    "        print( f\"Epochs: {self.hyperparams['epochs']}\" );\n",
    "        print( f\"Learning Rate: {self.hyperparams['lr']}\" );\n",
    "        print( f\"Minibatch Size: {self.hyperparams['minibatch_size']}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( f\"| Loss Function | : {self.loss_fn.name}\" );\n",
    "        print( \"---------------\" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        #print( f\"| Optimizer | : {self.optimizer.name}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( \"| Trainable Parameters | \")\n",
    "        print( \"---------------\" );\n",
    "        print( self.trainable_parameters );\n",
    "    \n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to add a layer to the network\n",
    "        \"\"\"\n",
    "        if isinstance( other, Layer ) or isinstance( other, InputLayer ):\n",
    "            # add weights between the layers, append the weight name to the list\n",
    "            self.weights.append( WeightLayer( self.layers[-1], other ) );\n",
    "            self.weight_names.append( self.weights[-1].name );\n",
    "            \n",
    "            # add the layer to the network, set the output layer to the last layer, and append the layer name to the list\n",
    "            self.layers.append( other );\n",
    "            self.output_layer = self.layers[-1];\n",
    "            self.layer_name.append( other.name );\n",
    "\n",
    "            # add the weights and biases to the trainable parameters\n",
    "            self.trainable_parameters[self.weights[-1].name] = self.weights[-1].weights;\n",
    "            self.trainable_parameters[other.name] = other.bias;\n",
    "\n",
    "            return self;\n",
    "    \n",
    "        if isinstance( other, Loss ):\n",
    "            self.loss_fn = other;\n",
    "            return self;\n",
    "\n",
    "        if isinstance( other, GradientDescent ):\n",
    "            self.optimizer = other;\n",
    "            return self;\n",
    "\n",
    "        else:\n",
    "            print( type( other ) );\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Calculate the output of the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.output_layer.output( inputs );\n",
    "\n",
    "    def forward( self, inputs ):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        fwd_output = {};\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            src_bias_add, src_activated, inputs = self.weights[i].output( inputs );\n",
    "            \n",
    "            fwd_output[self.weights[i].name] = inputs;\n",
    "            if self.layers[i].name == 'input':\n",
    "                continue;\n",
    "            fwd_output[self.layers[i].name] = src_activated;\n",
    "            fwd_output[self.layers[i].name + \"_bias_add\"] = src_bias_add;\n",
    "        \n",
    "        self.output( inputs );\n",
    "        fwd_output['output'] = self.output_layer.activated;\n",
    "        fwd_output['output_bias_add'] = self.output_layer.bias_add;\n",
    "        \n",
    "        return fwd_output;\n",
    "\n",
    "    def predict( self, X ):\n",
    "        \"\"\"\n",
    "        Predict the output of the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logits = self.forward( X );\n",
    "        return np.argmax( logits['output'], 0 );\n",
    "\n",
    "    def get_accuracy( self, predictions, Y ):\n",
    "        return np.sum( predictions == Y )/Y.size\n",
    "\n",
    "    def train( self, X, y ):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "            y : array\n",
    "            epochs : int\n",
    "            lr : float\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.losses_pd = pd.DataFrame( columns = ['fwd_losses'], index=[0] );\n",
    "        self.weights_pd = pd.DataFrame();\n",
    "        self.bias_pd = pd.DataFrame();\n",
    "        self.weight_layer_output_pd = pd.DataFrame();\n",
    "        self.activated_pd  = pd.DataFrame();\n",
    "        self.grad_pd = pd.DataFrame();\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            weights = self.weights[i];\n",
    "            for l in range ( weights.weights.shape[0] ):\n",
    "                for j in range( weights.weights.shape[1] ):\n",
    "                    self.weights_pd.loc[ 0 ,  weights.name + f\"{l}_{j}\" ] = weights.weights[l][j];\n",
    "        \n",
    "        for i in range( len( self.layers ) ):\n",
    "            layer = self.layers[i];\n",
    "            if hasattr( layer, 'bias' ):\n",
    "                for j in range( layer.bias.shape[0] ):\n",
    "                    self.bias_pd.loc[ 0, layer.name + f\"{j}\" ] = layer.bias[j];\n",
    "        \n",
    "        \n",
    "        #print( self.weights_pd )\n",
    "        #print( self.bias_pd )\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        for epoch in range( self.hyperparams['epochs'] ):\n",
    "  \n",
    "            # forward pass\n",
    "            #z1, a1, z2, a2 = self.forward( X );\n",
    "            fwd_outputs = self.forward( X );\n",
    "\n",
    "            #for item in fwd_outputs:\n",
    "                #print( item, fwd_outputs[item].shape );\n",
    "\n",
    "            # logging\n",
    "            #for name in self.weight_names:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                        #self.weight_layer_output_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "                \n",
    "            #for name in self.layer_name[1:]:\n",
    "                #for i in range( fwd_outputs[name].shape[0] ):\n",
    "                    #for j in range( fwd_outputs[name].shape[1] ):\n",
    "                       # self.activated_pd.loc[ 0, name + f\"{i}_{j}\" ] = fwd_outputs[name][i][j];\n",
    "        \n",
    "\n",
    "            # backward pass\n",
    "            #one_hot_labels = one_hot( y )\n",
    "            size = y.size;\n",
    "            \n",
    "            grads = BackProp()( X, y, size, fwd_outputs, self.trainable_parameters, self.layers[1].transfer_fx );\n",
    "            \n",
    "            # update the weights\n",
    "            self.trainable_parameters = self.optimizer.minimize( self.trainable_parameters, grads );\n",
    "\n",
    "            # update the weights and biases\n",
    "            for weight in self.weights:\n",
    "                weight.weights = self.trainable_parameters[weight.name];\n",
    "\n",
    "            for layer in self.layers[1:]:\n",
    "                layer.bias = self.trainable_parameters[layer.name];\n",
    "\n",
    "\n",
    "            if ( epoch+1 ) % int( self.hyperparams['epochs']/10 ) == 0:\n",
    "                print(f\"Iteration: {epoch+1} / {self.hyperparams['epochs']}\")\n",
    "                prediction = self.predict( X );\n",
    "                print(f'{self.get_accuracy(prediction, y):.3%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = load_data( \"xor\" );\n",
    "x_train, y_train = training[0], training[1];\n",
    "x_test, y_test = testing[0], testing[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\nSCALE_FACTOR = 255 # TRES IMPORTANT SINON OVERFLOW SUR EXP\\nWIDTH = x_train.shape[1]\\nHEIGHT = x_train.shape[2]\\nx_train = x_train.reshape(x_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\\nx_test = x_test.reshape(x_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\\n'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255 # TRES IMPORTANT SINON OVERFLOW SUR EXP\n",
    "WIDTH = x_train.shape[1]\n",
    "HEIGHT = x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
    "x_test = x_test.reshape(x_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "print( x_train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    #seed\n",
    "    \"seed\" : 42,\n",
    "\n",
    "    \"examples\" : x_train.shape[1],\n",
    "    \n",
    "    # model hyperparameters\n",
    "    \"input_units\" : x_train.shape[0],\n",
    "    \"hidden_units\" : 2,\n",
    "    \"output_units\" : 1,\n",
    "\n",
    "    # optimizer hyperparameters\n",
    "    \"lr\" : 0.15,\n",
    "\n",
    "    # training hyperparameters\n",
    "    \"epochs\" : 150,\n",
    "    \"minibatch_size\" : 4\n",
    "}\n",
    "#np.random.seed(params['seed']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print( f\"Training set: \\n {x_train}, {x_train.shape}\" );\n",
    "#print( f\"Labels: \\n {y_train}, {y_train.shape}\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = InputLayer( params, \"input\" );\n",
    "hidden_layer = Layer( params, \"hidden\", \"relu\" );\n",
    "output_layer = Layer( params, \"output\", \"softmax\" );\n",
    "\n",
    "loss = Loss( \"cross_entropy\" );\n",
    "optimizer = GradientDescent( params );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = input_layer >> hidden_layer >> output_layer >> loss >> optimizer;\n",
    "#nn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[406], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[0;32mIn[398], line 292\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    274\u001b[0m fwd_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward( X );\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m#for item in fwd_outputs:\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m#print( item, fwd_outputs[item].shape );\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m one_hot_labels \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m size \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize;\n\u001b[1;32m    295\u001b[0m grads \u001b[38;5;241m=\u001b[39m BackProp()( X, one_hot_labels, size, fwd_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtransfer_fx );\n",
      "Cell \u001b[0;32mIn[393], line 3\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m( Y ):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' return an 0 vector with 1 only in the position correspondind to the value in Y'''\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     one_hot_Y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#si le chiffre le plus grand dans Y est 9 ca fait 10 lignes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     one_hot_Y[Y,np\u001b[38;5;241m.\u001b[39marange(Y\u001b[38;5;241m.\u001b[39msize)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# met un 1 en ligne Y[i] et en colonne i, change l'ordre mais pas le nombre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m one_hot_Y\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "nn.train( x_train, y_train );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "print( \"Label:\", y_test[index] );\n",
    "current_image = x_test[:, index, None].reshape((WIDTH, HEIGHT)) * SCALE_FACTOR\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(current_image, interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "print( len ( nn.predict( x_test[:, index, None] ) ) );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
