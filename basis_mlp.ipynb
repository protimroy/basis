{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from tqdm import trange;\n",
    "import pandas as pd;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "print( np.__version__ );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_data():\n",
    "    input_array = np.array( [ [ 0.0, 0.0 ], [ 0.0, 1.0 ], [ 1.0, 0.0 ], [ 1.0, 1.0 ] ] );\n",
    "    output_array = np.array( [ [ 0.0 ], [ 1.0 ], [ 1.0 ], [ 0.0 ] ] );\n",
    "    return ( input_array.astype('float32'), output_array.astype('float32') ),\\\n",
    "( input_array.astype('float32'), output_array.astype('float32') );\n",
    "\n",
    "def load_data( name ):\n",
    "    if name == \"xor\":\n",
    "        return xor_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print( type( load_data( \"xor\" )[0][0] ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss( object ):\n",
    "    def __init__( self, name : str ):\n",
    "        self.name = name;\n",
    "\n",
    "    def __call__( self, y_pred, y_true ):\n",
    "        if self.name == \"mse\":\n",
    "            return np.mean( ( y_pred - y_true ) ** 2 );\n",
    "    \n",
    "        elif self.name == \"cross_entropy\":\n",
    "            return -np.mean( y_true * np.log( y_pred ) + ( 1 - y_true ) * np.log( 1 - y_pred ) );\n",
    "        \n",
    "        else:\n",
    "            raise ValueError( \"Invalid loss function\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackProp( object ):\n",
    "    def __init__( self, data : np.ndarray , labels : np.ndarray , trainable_parameters : dict, loss : Loss ):\n",
    "        self.data = data;\n",
    "        self.labels = labels;\n",
    "        self.trainable_parameters = trainable_parameters;\n",
    "        self.loss_fn = loss;\n",
    "\n",
    "    def __call__( self ):\n",
    "        # perform backwards computation\n",
    "\n",
    "        print( self.trainable_parameters );\n",
    "        #loss = self.loss_fn( y_pred, self.labels );\n",
    "        #return grads, loss;\n",
    "\n",
    "\n",
    "class GradientDescent(object):\n",
    "    \"\"\"\n",
    "    Gradient Descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters: dict):\n",
    "        self.parameters = parameters;\n",
    "        self.name = \"Gradient Descent\";\n",
    "    \n",
    "    def minimize( self, trainable_parameters, grads, loss_obj: Loss ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for key in trainable_parameters.keys():\n",
    "            trainable_parameters[key] -= self.parameters['learning_rate'] * grads[key];\n",
    "        return trainable_parameters;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer( object ):\n",
    "    \"\"\"\n",
    "    Base class for the Layer class.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name : str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Base Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.params : dict\n",
    "            self.node_no : int\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams;\n",
    "        self.name = name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Input layer for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Input Layer class.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            node_no : int\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.bias : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( InputLayer, self ).__init__( hyperparams, name );\n",
    "        self.node_no = hyperparams['input_units'];\n",
    "    \n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Mirror the inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return inputs;\n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to connect the layers.\n",
    "\n",
    "        args:\n",
    "            other : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return NeuralNetwork( self, other );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer( BaseLayer ):\n",
    "    \"\"\"\n",
    "    Layer class for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__( self, hyperparams : dict, name: str, transfer: str  ):\n",
    "        \"\"\"\n",
    "        Constructor for the Layer class.\n",
    "\n",
    "        args:\n",
    "            name : str\n",
    "            params : dict\n",
    "            node_no : int\n",
    "            transfer : str\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.name : str\n",
    "            self.transfer : str\n",
    "            self.inputs : object\n",
    "            self.outputs : object\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super( Layer, self ).__init__( hyperparams, name );\n",
    "        self.transfer = transfer;\n",
    "\n",
    "        self.inputs = None;\n",
    "        self.outputs = None;\n",
    "    \n",
    "        if \"hidden\" in name:\n",
    "            self.node_no = hyperparams['hidden_units'];\n",
    "        else:\n",
    "            self.node_no = hyperparams['output_units'];\n",
    "        \n",
    "        #create self.bias to be an array of size node_no with random values from a normal distribution between -1 and 1\n",
    "        self.bias = np.random.uniform( -1, 1, self.node_no );\n",
    "        \n",
    "        \n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to connect the layers.\n",
    "\n",
    "        args:\n",
    "            other : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return NeuralNetwork( self, other );\n",
    "\n",
    "    def __repr__( self ) -> str:\n",
    "        \"\"\"\n",
    "        Overwrite the __repr__ method to return the name of the layer.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            str\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.name;\n",
    "    \n",
    "    def transfer_fx( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Transfer function for the layer.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if deriv == True:\n",
    "            return inputs * ( 1 - inputs );\n",
    "        # implement the sigmoid transfer function\n",
    "        return 1 / ( 1 + np.exp( -inputs ) );\n",
    "\n",
    "\n",
    "    def output( self, inputs, deriv=False ):\n",
    "        \"\"\"\n",
    "        Calculate the output with the activation function and inputs.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            self.outputs : array\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        #self.activated = self.transfer_fx( self.inputs.output( inputs ) + self.bias );\n",
    "        if deriv == True:\n",
    "            self.activated = self.transfer_fx( inputs + self.bias, deriv=True );\n",
    "        else:\n",
    "            self.activated = self.transfer_fx( inputs + self.bias );\n",
    "        return self.activated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightLayer( BaseLayer):\n",
    "    \"\"\"\n",
    "    Weight layer for the perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, src : Layer, dest : Layer ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the weight layer.\n",
    "\n",
    "        args:\n",
    "            params : dict\n",
    "            src : Layer\n",
    "            dest : Layer\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.src : Layer\n",
    "            self.dest : Layer\n",
    "            self.input_size : tuple\n",
    "            self.output_size : tuple\n",
    "            self.name : str\n",
    "            self.weights : array\n",
    "\n",
    "        raises: \n",
    "            None\n",
    "        \"\"\"\n",
    "        self.src = src;\n",
    "        self.dest = dest;\n",
    "\n",
    "        #print( f\"src: {src.node_no}, dest: {dest.node_no}\" );\n",
    "    \n",
    "        #self.weights = np.random.randn( self.src.node_no, self.dest.node_no );\n",
    "        #create self.weights to be a matrix of size src.node_no x dest.node_no with random values from a uniform distribution between -1 and 1\n",
    "        self.weights = np.random.uniform( -1, 1, ( self.src.node_no, self.dest.node_no ) );\n",
    "        self.name    = \"W_%s_%s_layer\" % ( self.src.name, self.dest.name );\n",
    "\n",
    "        self.src.outputs = self;\n",
    "        self.dest.inputs = self;\n",
    "\n",
    "    def __repr__( self ) -> str:\n",
    "        \"\"\"\n",
    "        Overwrite the __repr__ method to return the name of the layer.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            str\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.name;\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Matrix multiplication between the inputs and the weights.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"        \n",
    "        return self.src.output( inputs ) @ self.weights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork( object ):\n",
    "    \"\"\"\n",
    "    This class respresents a Neural Networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, layer0, layer1 ):\n",
    "        \"\"\"\n",
    "        Constructor for the Neural Network class.\n",
    "        Creates a network with an input layer, layer0 and an output layer, layer1.\n",
    "\n",
    "        args:\n",
    "            layer0 : object\n",
    "            layer1 : object\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            self.hyperparams          : ( dict ) hyperparameters for the network.\n",
    "            self.layers               : ( list ) list of layers in the network.\n",
    "            self.layer_name           : ( list ) list of names of the layers in the network.\n",
    "            self.input_layer          : ( object ) input layer of the network.\n",
    "            self.output_layer         : ( object ) output layer of the network.\n",
    "            self.weights              : ( list ) list of weights in the network.\n",
    "            self.weight_names         : ( list ) list of names of the weights in the network.\n",
    "            self.loss_fn              : ( object ) loss function for the network.\n",
    "            self.optimizer            : ( object ) optimizer for the network.\n",
    "            self.learnable_parameters : ( dict ) dictionary of learnable parameters in the network.\n",
    "\n",
    "        raises:\n",
    "            None \n",
    "        \"\"\"\n",
    "        # hyperparameters dictionary\n",
    "        self.hyperparams = layer0.hyperparams;\n",
    "\n",
    "        # layers\n",
    "        self.layers = [ layer0, layer1 ];\n",
    "        self.layer_name = [ layer0.name, layer1.name ];\n",
    "        self.input_layer = layer0;\n",
    "        self.output_layer = self.layers[-1];\n",
    "\n",
    "        # weights\n",
    "        self.weights = [ WeightLayer( layer0, layer1 ) ];\n",
    "        self.weight_names = [ self.weights[0].name ];\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fn = None;\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = None;\n",
    "\n",
    "        # initialize and populate learnable parameters used for learning\n",
    "        self.trainable_parameters = {};\n",
    "        for weight in self.weights:\n",
    "            self.trainable_parameters[weight.name] = weight.weights;\n",
    "        for layer in self.layers[1:]:\n",
    "            self.trainable_parameters[layer.name] = layer.bias;\n",
    "\n",
    "        \n",
    "\n",
    "    def init_logs( self ):\n",
    "        self.logs = pd.DataFrame();\n",
    "\n",
    "        # set the columns of the dataframe\n",
    "        self.logs = pd.DataFrame( columns = [\n",
    "            #primary key \n",
    "            'epoch', \n",
    "\n",
    "            #weights\n",
    "            'weight_name', \n",
    "            'weight_node_no',  \n",
    "            'weight_value', \n",
    "            \n",
    "            #layer\n",
    "            'layer_name', \n",
    "            'layer_node_no',\n",
    "            'bias',\n",
    "\n",
    "            #matmul\n",
    "            'pre_activation',\n",
    "\n",
    "            #activations\n",
    "            'activated',\n",
    "            \n",
    "            'hidden_losses', \n",
    "            'output_losses' \n",
    "            \n",
    "            ] );\n",
    "\n",
    "    \n",
    "    def summary( self ):\n",
    "        \"\"\"\n",
    "        Print the summary of the network.\n",
    "\n",
    "        args:\n",
    "            None\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print( \"-------\" );\n",
    "        print( \"| Summary |\" );\n",
    "        print( \"-------\" );\n",
    "        print( f\"Input Layer: { self.input_layer.node_no }\" );\n",
    "        print( f\"Hidden Layer: { self.layers[1].node_no }\" );\n",
    "        print( f\"Output Layer: { self.output_layer.node_no }\" );\n",
    "    \n",
    "        print( \"-------\" );\n",
    "        print( \"| Weights |\" );\n",
    "        print( \"-------\" );\n",
    "        for weight_obj in self.weights:\n",
    "            print( f\"{weight_obj.name}: \\n {weight_obj.weights} , {weight_obj.weights.shape}\" );\n",
    "        \n",
    "        print( \"-------\" );\n",
    "        print( \"| Bias |\" );\n",
    "        print( \"------\" );\n",
    "        for layer_obj in self.layers:\n",
    "            if hasattr( layer_obj, 'bias' ):\n",
    "                print( f\"{layer_obj.name}: \\n {layer_obj.bias}, {layer_obj.bias.shape}\" );\n",
    "            else:\n",
    "                print( f\"{layer_obj.name}: \" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        print( \"| Hyperparameters |\" );\n",
    "        print( \"---------------\" );\n",
    "        print( f\"Epochs: {self.hyperparams['epochs']}\" );\n",
    "        print( f\"Learning Rate: {self.hyperparams['lr']}\" );\n",
    "        print( f\"Minibatch Size: {self.hyperparams['minibatch_size']}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( f\"| Loss Function | : {self.loss_fn.name}\" );\n",
    "        print( \"---------------\" );\n",
    "        \n",
    "        print( \"---------------\" );\n",
    "        #print( f\"| Optimizer | : {self.optimizer.name}\" );\n",
    "        print( \"---------------\" );\n",
    "\n",
    "        print( \"---------------\" );\n",
    "        print( \"| Trainable Parameters | \")\n",
    "        print( \"---------------\" );\n",
    "        print( self.trainable_parameters );\n",
    "    \n",
    "\n",
    "    def __rshift__( self, other ):\n",
    "        \"\"\"\n",
    "        Overwrite the right shift operator to add a layer to the network\n",
    "        \"\"\"\n",
    "        if isinstance( other, Layer ) or isinstance( other, InputLayer ):\n",
    "            # add weights between the layers, append the weight name to the list\n",
    "            self.weights.append( WeightLayer( self.layers[-1], other ) );\n",
    "            self.weight_names.append( self.weights[-1].name );\n",
    "            \n",
    "            # add the layer to the network, set the output layer to the last layer, and append the layer name to the list\n",
    "            self.layers.append( other );\n",
    "            self.output_layer = self.layers[-1];\n",
    "            self.layer_name.append( other.name );\n",
    "\n",
    "            # add the weights and biases to the trainable parameters\n",
    "            self.trainable_parameters[self.weights[-1].name] = self.weights[-1].weights;\n",
    "            self.trainable_parameters[other.name] = other.bias;\n",
    "\n",
    "            return self;\n",
    "    \n",
    "        if isinstance( other, Loss ):\n",
    "            self.loss_fn = other;\n",
    "            return self;\n",
    "\n",
    "        if isinstance( other, GradientDescent ):\n",
    "            self.optimizer = other;\n",
    "            return self;\n",
    "\n",
    "        else:\n",
    "            print( type( other ) );\n",
    "\n",
    "    def output( self, inputs ):\n",
    "        \"\"\"\n",
    "        Calculate the output of the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.output_layer.output( inputs );\n",
    "\n",
    "    def forward( self, inputs ):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        args:\n",
    "            inputs : array\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        fwd_output = {};\n",
    "\n",
    "        for i in range( len( self.weights ) ):\n",
    "            # log weights and biases;\n",
    "\n",
    "            pre_activation = self.weights[i].output( inputs );\n",
    "            activated_weights = self.layers[i + 1].output( pre_activation );\n",
    "            #fwd_output[self.layers[i+1].name] = { 'pre_activated' : pre_activation, \n",
    "            #                                       'activated' : activated_weights \n",
    "            #                                      };\n",
    "            fwd_output[self.layers[i+1].name] = activated_weights;\n",
    "            fwd_output[self.weights[i].name] = pre_activation;\n",
    "\n",
    "            inputs = activated_weights;\n",
    "\n",
    "        return fwd_output;\n",
    "\n",
    "    def predict( self, X ):\n",
    "        \"\"\"\n",
    "        Predict the output of the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "\n",
    "        returns:\n",
    "            array\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.forward( X );\n",
    "\n",
    "    def train( self, X, y ):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "\n",
    "        args:\n",
    "            X : array\n",
    "            y : array\n",
    "            epochs : int\n",
    "            lr : float\n",
    "\n",
    "        returns:\n",
    "            None\n",
    "\n",
    "        attributes:\n",
    "            None\n",
    "\n",
    "        raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "         # Define the shapes for each log entry\n",
    "        log_shapes = {\n",
    "\n",
    "            \"hidden_losses\" : (1,1),\n",
    "            \"output_losses\" : (1,)\n",
    "\n",
    "            # rewrite the above as a dict comprehension\n",
    "            #\"weights\" : { weight.name : weight.weights.shape for weight in self.weights },\n",
    "            #\"biases\" : { layer.name : layer.bias.shape for layer in self.layers[1:] },\n",
    "\n",
    "            #\"pre_activated\" : { layer.name : (self.hyperparams['epochs'],) for layer in self.layers[1:] },\n",
    "            #\"activated\" : { layer.name : (self.hyperparams['epochs'],) for layer in self.layers[1:] },\n",
    "\n",
    "            \n",
    "            #\"weight_gradients\" : { weight.name : weight.weights.shape for weight in self.weights },\n",
    "            #\"bias_gradients\" : { layer.name : layer.bias.shape for layer in self.layers[1:] }\n",
    "\n",
    "        }\n",
    "\n",
    "        # Initialize the logs with zeros\n",
    "        self.logs = { key: np.empty( ( self.hyperparams['epochs'],1 ) , dtype=np.float32) for key, shape in log_shapes.items()}\n",
    "\n",
    "        #initalize the logger\n",
    "\n",
    "        #print( self.logs )\n",
    "\n",
    "        for epoch in trange( self.hyperparams['epochs']+2 ):\n",
    "  \n",
    "            # forward pass\n",
    "            fwd_outputs = self.forward( X );\n",
    "\n",
    "            #print( f\"fwd_weights: {fwd_weights}\" )\n",
    "\n",
    "            # calculate the loss for each layer\n",
    "            for key, value in fwd_outputs.items():\n",
    "                if key in self.layer_name[1:]:\n",
    "                    if epoch == 0:\n",
    "                        print(  np.vstack(  [self.loss_fn( value, y )] ) )\n",
    "                        self.logs[key+'_losses'] = np.vstack(  [self.loss_fn( value, y )] );\n",
    "                    else:\n",
    "                        self.logs[key+'_losses'] = np.append( self.logs[key+'_losses'], np.vstack(  [self.loss_fn( value, y )] ), axis=0 );\n",
    "                    \n",
    "\n",
    "                        \n",
    "                #hidden_loss[key] = self.loss_fn( value['activated'], y );\n",
    "\n",
    "        print( self.logs )\n",
    "            # logging\n",
    "            #self.logs['output_losses'][epoch] = self.loss_fn( fwd_weights['output']['activated'], y );\n",
    "            #self.logs['hidden_losses'][epoch] = hidden_loss;\n",
    "\n",
    "            #print( f\"Epoch: { epoch + 1 }, Hidden_L Loss: { self.logs['hidden_losses'][epoch]['hidden'] } , Output_L Loss: { self.logs['output_losses'][epoch] }\" );\n",
    "\n",
    "            # backward pass\n",
    "            grad, loss = BackProp( X, y, self.trainable_parameters, self.loss_fn )();\n",
    "\n",
    "            #print( f\"Epoch: { epoch + 1 }, Hidden_L Loss: { hidden_losses[epoch]['hidden'] } , Output_L Loss: { output_loss[epoch] }\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # model hyperparameters\n",
    "    \"input_units\" : 2,\n",
    "    \"hidden_units\" : 2,\n",
    "    \"output_units\" : 1,\n",
    "\n",
    "    # optimizer hyperparameters\n",
    "    \"lr\" : 0.01,\n",
    "\n",
    "    # training hyperparameters\n",
    "    \"epochs\" : 1,\n",
    "    \"minibatch_size\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = load_data( \"xor\" );\n",
    "x_train, y_train = training[0], training[1];\n",
    "x_test, y_test = testing[0], testing[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: \n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]], (4, 2)\n",
      "Labels: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]], (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print( f\"Training set: \\n {x_train}, {x_train.shape}\" );\n",
    "print( f\"Labels: \\n {y_train}, {y_train.shape}\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = InputLayer( params, \"input\" );\n",
    "hidden_layer = Layer( params, \"hidden\", \"sigmoid\" );\n",
    "output_layer = Layer( params, \"output\", \"sigmoid\" );\n",
    "\n",
    "loss = Loss( \"cross_entropy\" );\n",
    "optimizer = GradientDescent( params );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "| Summary |\n",
      "-------\n",
      "Input Layer: 2\n",
      "Hidden Layer: 2\n",
      "Output Layer: 1\n",
      "-------\n",
      "| Weights |\n",
      "-------\n",
      "W_input_hidden_layer: \n",
      " [[-0.34563078  0.62021313]\n",
      " [-0.65881599  0.07018818]] , (2, 2)\n",
      "W_hidden_output_layer: \n",
      " [[ 0.11830519]\n",
      " [-0.1223834 ]] , (2, 1)\n",
      "-------\n",
      "| Bias |\n",
      "------\n",
      "input: \n",
      "hidden: \n",
      " [-0.93997788 -0.34924307], (2,)\n",
      "output: \n",
      " [-0.4860153], (1,)\n",
      "---------------\n",
      "| Hyperparameters |\n",
      "---------------\n",
      "Epochs: 1\n",
      "Learning Rate: 0.01\n",
      "Minibatch Size: 4\n",
      "---------------\n",
      "---------------\n",
      "| Loss Function | : cross_entropy\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "| Trainable Parameters | \n",
      "---------------\n",
      "{'W_input_hidden_layer': array([[-0.34563078,  0.62021313],\n",
      "       [-0.65881599,  0.07018818]]), 'hidden': array([-0.93997788, -0.34924307]), 'W_hidden_output_layer': array([[ 0.11830519],\n",
      "       [-0.1223834 ]]), 'output': array([-0.4860153])}\n"
     ]
    }
   ],
   "source": [
    "nn = input_layer >> hidden_layer >> output_layer >> loss;\n",
    "nn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2268.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82464778]]\n",
      "[[0.72584187]]\n",
      "{'hidden_losses': array([[0.82464778],\n",
      "       [0.82464778],\n",
      "       [0.82464778]]), 'output_losses': array([[0.72584187],\n",
      "       [0.72584187],\n",
      "       [0.72584187]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nn.train( x_train, y_train );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict( x_test );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
